{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f02d4a",
   "metadata": {},
   "source": [
    "# Text Mining & Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af9041",
   "metadata": {},
   "source": [
    "### *Objective*:\n",
    "To apply text mining techniques to perform document classification. You will train a machine learning model to distinguish between two types of posts from Reddit: those related to Data Science and those related to Game of Thrones. The goal is to explore how text mining can be used for categorizing documents and gain insights into real-world applications like spam filtering, sentiment analysis, and topic detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c0937",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93d58d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/roxel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/roxel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/roxel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/roxel/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %pip install -q wordcloud nltk seaborn matplotlib pandas numpy\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a9a1d",
   "metadata": {},
   "source": [
    "Since I couldn't find an existing dataset that has reddit posts about both, data_science & GOT, I've decided to make things a little interesting and creating my own dataset from existing datasets on reddit posts about data_science & GOT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94358088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download the datasets from Kaggle\n",
    "# path_a = kagglehub.dataset_download(\"nikhilkhetan/game-of-thrones\")\n",
    "# print(\"Path to GOT dataset:\", path_a)\n",
    "\n",
    "# path_b = kagglehub.dataset_download(\"maksymshkliarevskyi/reddit-data-science-posts\")\n",
    "# print(\"Path to DataSci dataset:\", path_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa7fd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "path_a = '../data/GameofThrones.csv'\n",
    "path_b = '../data/reddit_database.csv'\n",
    "\n",
    "got_df = pd.read_csv(path_a)\n",
    "ds_df = pd.read_csv(path_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14df375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['created_date', 'created_timestamp', 'subreddit', 'title', 'id',\n",
      "       'author', 'author_created_utc', 'full_link', 'score', 'num_comments',\n",
      "       'num_crossposts', 'subreddit_subscribers', 'post'],\n",
      "      dtype='object')\n",
      "Index(['title', 'score', 'id', 'url', 'comms_num', 'created', 'body',\n",
      "       'timestamp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# inspecting columns to merge both datasets\n",
    "print(ds_df.columns)\n",
    "print(got_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d7981",
   "metadata": {},
   "source": [
    "Matching Columns: {title, id, score, post:body}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb8c7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['text'] = ds_df['title'].fillna('') + ' ' + ds_df['post'].fillna('')\n",
    "got_df['text'] = got_df['title'].fillna('') + ' ' + got_df['body'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b39f8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df = ds_df[ds_df['text'].str.strip() != '']\n",
    "got_df = got_df[got_df['text'].str.strip() != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a6727242",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = min(len(ds_df), len(got_df), 5000)  # to select a minimum of 5000 samples from each dataset or less if available\n",
    "ds_sample = ds_df.sample(n=n, random_state=42)[['text']].copy()\n",
    "got_sample = got_df.sample(n=n, random_state=42)[['text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aab53548",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sample['category'] = 'data science'\n",
    "got_sample['category'] = 'game of thrones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "896788ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.concat([ds_sample, got_sample], ignore_index=True)\n",
    "base = base.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2536b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.to_csv('../data/reddit_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0466710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "# df = pd.read_csv('../data/reddit_posts.csv')\n",
    "# not needed in my case right now, but keeping it for future reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc84abb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3918, 2)\n",
      "Dataset columns: Index(['text', 'category'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3918 entries, 0 to 3917\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      3918 non-null   object\n",
      " 1   category  3918 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 61.3+ KB\n",
      "Dataset info: None\n",
      "                                                text         category\n",
      "0  Seeking resources to have a deeper understandi...     data science\n",
      "1  [Spoilers] Finally, someone says that Daenerys...  game of thrones\n",
      "2  Modelling process for logistic regression I'm ...     data science\n",
      "3         [Spoilers] The 3 Sides of Jamie Lannister   game of thrones\n",
      "4  [NO SPOILERS] is it just me or are knights nev...  game of thrones\n"
     ]
    }
   ],
   "source": [
    "# analyzing the dataset\n",
    "print(\"Dataset shape:\", base.shape)\n",
    "print(\"Dataset columns:\", base.columns)\n",
    "print(\"Dataset info:\", base.info())\n",
    "print(base.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58142881",
   "metadata": {},
   "source": [
    "Since I, myself created the dataset, I know that it doesn't have any null-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8846b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "     \"\"\"\n",
    "     Remove emojis from the given text.\n",
    "     \"\"\"\n",
    "     emoji_pattern = re.compile(\n",
    "     \"[\"\n",
    "     u\"\\U0001F600-\\U0001F64F\" # Emoticons\n",
    "     u\"\\U0001F300-\\U0001F5FF\" # Symbols & pictographs\n",
    "     u\"\\U0001F680-\\U0001F6FF\" # Transport & map symbols\n",
    "     u\"\\U0001F700-\\U0001F77F\" # Alchemical symbols\n",
    "     u\"\\U0001F780-\\U0001F7FF\" # Geometric shapes extended\n",
    "     u\"\\U0001F800-\\U0001F8FF\" # Supplemental arrows-C\n",
    "     u\"\\U0001F900-\\U0001F9FF\" # Supplemental symbols & pictographs\n",
    "     u\"\\U0001FA00-\\U0001FA6F\" # Chess symbols\n",
    "     u\"\\U0001FA70-\\U0001FAFF\" # Symbols and pictographs extended-A\n",
    "     u\"\\U00002702-\\U000027B0\" # Dingbats\n",
    "     u\"\\U000024C2-\\U0001F251\" # Enclosed characters\n",
    "     \"]+\",\n",
    "     flags=re.UNICODE,\n",
    "     )\n",
    "     return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a4609518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to preprocess the data\n",
    "def preprocess_text(text):\n",
    "     \"\"\"\n",
    "     Perform preprocessing steps on a given text.\n",
    "     \"\"\"\n",
    "     # Lowercase the text\n",
    "     text = text.lower()\n",
    "     # Remove URLs\n",
    "     text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "     # Remove emojis\n",
    "     text = remove_emojis(text)\n",
    "     # Remove punctuation and special characters\n",
    "     text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "     # Tokenize the text\n",
    "     tokens = nltk.word_tokenize(text)\n",
    "     # Remove stopwords\n",
    "     stop_words = set(stopwords.words('english'))\n",
    "     tokens = [word for word in tokens if word not in stop_words]\n",
    "     # Lemmatize tokens\n",
    "     lemmatizer = WordNetLemmatizer()\n",
    "     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "     # Rejoin tokens into a single string\n",
    "     return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "870994a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned dataset\n",
      "=================\n",
      "                                                text         category\n",
      "0  seeking resource deeper understanding cnns int...     data science\n",
      "1  spoiler finally someone say daeneryss arc nowhere  game of thrones\n",
      "2  modelling process logistic regression im wonde...     data science\n",
      "3                       spoiler side jamie lannister  game of thrones\n",
      "4  spoiler knight never used properly game throne...  game of thrones\n",
      "5            comment nothing memorable winter coming  game of thrones\n",
      "6  hey everyone made gaze tracker thought id shar...     data science\n",
      "7  comment submission automatically removed post ...  game of thrones\n",
      "8              top highest paying technology company     data science\n",
      "9  comment polarized ending internet youre going ...  game of thrones\n"
     ]
    }
   ],
   "source": [
    "clean_df= base\n",
    "clean_df['text'] = clean_df['text'].apply(preprocess_text)\n",
    "print(\"cleaned dataset\")\n",
    "print(\"=================\")\n",
    "print(clean_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f8d06",
   "metadata": {},
   "source": [
    "I'm label-encoding the category column because it's binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96acbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "clean_df['label'] = le.fit_transform(clean_df['category'])\n",
    "# 0: 'data science', 1: 'game of thrones'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37f4c285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text         category  label\n",
      "0  seeking resource deeper understanding cnns int...     data science      0\n",
      "1  spoiler finally someone say daeneryss arc nowhere  game of thrones      1\n",
      "2  modelling process logistic regression im wonde...     data science      0\n",
      "3                       spoiler side jamie lannister  game of thrones      1\n",
      "4  spoiler knight never used properly game throne...  game of thrones      1\n",
      "5            comment nothing memorable winter coming  game of thrones      1\n",
      "6  hey everyone made gaze tracker thought id shar...     data science      0\n",
      "7  comment submission automatically removed post ...  game of thrones      1\n",
      "8              top highest paying technology company     data science      0\n",
      "9  comment polarized ending internet youre going ...  game of thrones      1\n"
     ]
    }
   ],
   "source": [
    "print(clean_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5d3197a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction: Convert text into TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(clean_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4db7fee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(vectorizer, '../data/tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d364dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Labels (Data Science = 0, Game of Thrones = 1)\n",
    "y = clean_df['category'].map({'data science': 0, 'game of thrones': 1})\n",
    "\n",
    "y = y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "297be2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a40815b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = {\n",
    "     'naive_bayes': MultinomialNB(),\n",
    "     'logistic_regression': LogisticRegression(max_iter=1200),\n",
    "     'svm': SVC(kernel='linear')  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "12b18c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def train_and_select_best_model(models):\n",
    "     best_model = None\n",
    "     best_f1 = 0\n",
    "     results = []\n",
    "\n",
    "     # Step 3: Train and evaluate each model\n",
    "     for name, model in models.items():\n",
    "          model.fit(X_train, y_train)\n",
    "          y_pred = model.predict(X_test)\n",
    "\n",
    "          acc = accuracy_score(y_test, y_pred)\n",
    "          prec = precision_score(y_test, y_pred)\n",
    "          rec = recall_score(y_test, y_pred)\n",
    "          f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "          results.append((name, acc, prec, rec, f1))\n",
    "\n",
    "          print(f\"\\n📊 {name}\")\n",
    "          print(f\"  Accuracy:  {acc:.4f}\")\n",
    "          print(f\"  Precision: {prec:.4f}\")\n",
    "          print(f\"  Recall:    {rec:.4f}\")\n",
    "          print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "          # Update best model\n",
    "          if f1 > best_f1:\n",
    "               best_f1 = f1\n",
    "               best_model = model\n",
    "     \n",
    "     print(\"\\n✅ Best model based on F1-score:\", type(best_model).__name__)\n",
    "     return best_model, vectorizer, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce720915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 naive_bayes\n",
      "  Accuracy:  0.9796\n",
      "  Precision: 0.9623\n",
      "  Recall:    0.9974\n",
      "  F1 Score:  0.9795\n",
      "\n",
      "📊 logistic_regression\n",
      "  Accuracy:  0.9974\n",
      "  Precision: 0.9974\n",
      "  Recall:    0.9974\n",
      "  F1 Score:  0.9974\n",
      "\n",
      "📊 svm\n",
      "  Accuracy:  0.9987\n",
      "  Precision: 0.9974\n",
      "  Recall:    1.0000\n",
      "  F1 Score:  0.9987\n",
      "\n",
      "✅ Best model based on F1-score: SVC\n"
     ]
    }
   ],
   "source": [
    "best_model, vectorizer, model_results = train_and_select_best_model(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f920a7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/label_encoder.pkl']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save best model\n",
    "joblib.dump(best_model, '../data/best_model.pkl')\n",
    "# save label encoder\n",
    "joblib.dump(le, '../data/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7669d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load saved model, vectorizer, and label encoder\n",
    "with open('../data/best_model.pkl', 'rb') as f:\n",
    "     model = joblib.load(f)\n",
    "\n",
    "with open('../data/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "     vectorizer = joblib.load(f)\n",
    "\n",
    "with open('../data/label_encoder.pkl', 'rb') as f:\n",
    "     label_encoder = joblib.load(f)\n",
    "\n",
    "# Define your prediction function\n",
    "def predict_category(texts):\n",
    "     \"\"\"\n",
    "     Takes a list of raw text strings, vectorizes them, and returns predicted categories.\n",
    "     \"\"\"\n",
    "     if isinstance(texts, str):\n",
    "          texts = [texts]  # convert single string to list\n",
    "\n",
    "     # Vectorize the input texts\n",
    "     X_new = vectorizer.transform(texts)\n",
    "\n",
    "     # Predict label\n",
    "     predictions = model.predict(X_new)\n",
    "\n",
    "     # Decode label to original category name\n",
    "     categories = label_encoder.inverse_transform(predictions)\n",
    "     return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db623edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data science']\n",
      "['game of thrones' 'data science']\n"
     ]
    }
   ],
   "source": [
    "# single prediction\n",
    "print(predict_category(\"How to build a neural network with PyTorch?\"))\n",
    "\n",
    "# batch predictions\n",
    "test_posts = [\n",
    "     \"Spoiler alert: Arya kills the Night King!\",\n",
    "     \"Best way to visualize time series data using Python?\"\n",
    "]\n",
    "\n",
    "print(predict_category(test_posts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79551039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
