{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f02d4a",
   "metadata": {},
   "source": [
    "# Text Mining & Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af9041",
   "metadata": {},
   "source": [
    "### *Objective*:\n",
    "To apply text mining techniques to perform document classification. You will train a machine learning model to distinguish between two types of posts from Reddit: those related to Data Science and those related to Game of Thrones. The goal is to explore how text mining can be used for categorizing documents and gain insights into real-world applications like spam filtering, sentiment analysis, and topic detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c0937",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d58d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/roxel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/roxel/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/roxel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/roxel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%pip install -q wordcloud nltk seaborn matplotlib pandas numpy\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a9a1d",
   "metadata": {},
   "source": [
    "Since I couldn't find an existing dataset that has reddit posts about both, data_science & GOT, I've decided to make things a little interesting and creating my own dataset from existing datasets on reddit posts about data_science & GOT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94358088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/roxel/anaconda3/envs/env/lib/python3.13/site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/nikhilkhetan/game-of-thrones?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 339k/339k [00:00<00:00, 428kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to GOT dataset: /home/roxel/.cache/kagglehub/datasets/nikhilkhetan/game-of-thrones/versions/1\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/maksymshkliarevskyi/reddit-data-science-posts?dataset_version_number=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114M/114M [00:32<00:00, 3.70MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to DataSci dataset: /home/roxel/.cache/kagglehub/datasets/maksymshkliarevskyi/reddit-data-science-posts/versions/4\n"
     ]
    }
   ],
   "source": [
    "# %pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download the datasets from Kaggle\n",
    "path_a = kagglehub.dataset_download(\"nikhilkhetan/game-of-thrones\")\n",
    "print(\"Path to GOT dataset:\", path_a)\n",
    "\n",
    "path_b = kagglehub.dataset_download(\"maksymshkliarevskyi/reddit-data-science-posts\")\n",
    "print(\"Path to DataSci dataset:\", path_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7fd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "path_a = '../data/GameofThrones.csv'\n",
    "path_b = '../data/reddit_database.csv'\n",
    "\n",
    "got_df = pd.read_csv(path_a)\n",
    "ds_df = pd.read_csv(path_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14df375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['created_date', 'created_timestamp', 'subreddit', 'title', 'id',\n",
      "       'author', 'author_created_utc', 'full_link', 'score', 'num_comments',\n",
      "       'num_crossposts', 'subreddit_subscribers', 'post'],\n",
      "      dtype='object')\n",
      "Index(['title', 'score', 'id', 'url', 'comms_num', 'created', 'body',\n",
      "       'timestamp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# inspecting columns to merge both datasets\n",
    "print(ds_df.columns)\n",
    "print(got_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d7981",
   "metadata": {},
   "source": [
    "Matching Columns: {title, id, score, post:body}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8c7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['text'] = ds_df['title'].fillna('') + ' ' + ds_df['post'].fillna('')\n",
    "got_df['text'] = got_df['title'].fillna('') + ' ' + got_df['body'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39f8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df = ds_df[ds_df['text'].str.strip() != '']\n",
    "got_df = got_df[got_df['text'].str.strip() != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6727242",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = min(len(ds_df), len(got_df), 5000)  # Selecting a minimum of 5000 samples from each dataset or less if available\n",
    "ds_sample = ds_df.sample(n=n, random_state=42)[['text']].copy()\n",
    "got_sample = got_df.sample(n=n, random_state=42)[['text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab53548",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sample['category'] = 'data science'\n",
    "got_sample['category'] = 'game of thrones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896788ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.concat([ds_sample, got_sample], ignore_index=True)\n",
    "base = base.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2536b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.to_csv('../data/reddit_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "# df = pd.read_csv('../data/reddit_posts.csv')\n",
    "# not needed in my case right now, but keeping it for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84abb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
